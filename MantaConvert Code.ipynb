{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gylxZkMmAjga"
      },
      "source": [
        "\n",
        "#Introduction to the MANTA Conversion Notebook\n",
        "The MANTA Conversion Notebook serves as a comprehensive tool to simplify the process of converting soundscape data collected by MANTA devices into a standardized netCDF format, aligned with the NCEI community standard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw1N0zIbNtaF"
      },
      "source": [
        "This code installs libraries for working with netCDF files and data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chktE8xnAjgd",
        "outputId": "a69c5bfa-8d0c-4b61-da18-e02e718c4867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2024.12.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.26.4)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (2024.11.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from xarray) (1.26.4)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.10/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install netCDF4\n",
        "!pip install xarray\n",
        "!pip install numpy\n",
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhnQPhWAjgc"
      },
      "source": [
        "##Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXX7O1DvOS99"
      },
      "source": [
        "This code imports essential libraries and modules for handling files, data processing, and netCDF operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm8xCAWJEUQd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from configparser import ConfigParser\n",
        "import shutil\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from datetime import datetime, date\n",
        "from netCDF4 import date2num, num2date\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpQYpLGRDvwv"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Connecting Google Drive to Google Colab\n",
        "\n",
        "This code connects your Google Drive to Colab, so you can easily access and save files directly from your Drive while working in the Colab environment.\n",
        "\n",
        "**Please ensure that your directory is stored in the same account with which you login in the google drive**\n",
        "\n",
        "\n",
        "#### **Step 1: Granting Access to Google Drive**\n",
        "\n",
        "When you run the code to mount your Google Drive in Colab, a new window will pop up asking you to authorize access. Here's what you need to know:\n",
        "\n",
        "1. **Authorization Screen**:  \n",
        "   You will see a Google sign-in screen that asks you to select or log in to the Google account associated with your Drive.  \n",
        "   \n",
        "   **Important**: Ensure that the directory or folder you want to access is stored in the same Google account you sign in with.\n",
        "\n",
        "2. **Permissions Required**:  \n",
        "   You will need to grant the following permissions for Colab to connect to your Drive:\n",
        "   - **Access your Google Drive files**: This is required for reading and saving files to your Drive.\n",
        "   - **Manage the contents of your Google Drive**: This permission allows Colab to create, update, or delete files as needed.\n",
        "\n",
        "3. **Verification Code**:  \n",
        "   After granting access, you will receive an authorization code. Copy this code and paste it into the Colab prompt to complete the connection.\n",
        "\n",
        "\n",
        "\n",
        "#### **Step 2: Note for NOAA Google Accounts**\n",
        "\n",
        "If you are using a Google account provided by NOAA (or other organization-specific accounts), this method will **not work** due to restrictions imposed by the account administrator. Use a personal Google account instead.\n",
        "\n",
        "\n",
        "\n",
        "#### **Step 3: After Successfully Connecting**\n",
        "\n",
        "Once the connection is established, you will see a folder named `drive` appear in the Colab file browser on the left-hand side. This represents your Google Drive, and you can navigate through it just like you would in Google Drive itself.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ythsdfmx9WTU",
        "outputId": "57cc4174-586b-4758-e84d-14a891fe7810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entering the directory to get started with the conversion"
      ],
      "metadata": {
        "id": "-LSM0HRmkHum"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFOwMNZLEz4s"
      },
      "source": [
        "\n",
        "\n",
        "##Directory Structure and Input Guide\n",
        "\n",
        "\n",
        "**Purpose**: This script processes subdirectories within a main directory containing audio data and metadata files. Follow the structure and instructions below to ensure proper execution.\n",
        "\n",
        "**Please ensure the directory structure and folder names are same as mentioned below. The code will not work if the file structure isn't identical to the one given here**\n",
        "\n",
        "#### 1. Directory Structure\n",
        "Your main directory (e.g., `SoundScape Conversion`) should have:\n",
        "- A `settings file` folder containing `settings.json` (shared across all subdirectories).\n",
        "- Subdirectories (e.g., `NRS01A`, `NRS01B`) with the following contents:\n",
        "  - **data**: Folder containing CSV files.\n",
        "  - **metadata**: Folder containing metadata files.\n",
        "  - **calibration**: Folder containing calibration files.\n",
        "  - **output**: Folder (created by the script) where results will be saved.\n",
        "\n",
        "**Example**:\n",
        "```\n",
        "SoundScape Conversion/\n",
        "├── settings file/\n",
        "│   ├── settings.json\n",
        "├── NRS01A/\n",
        "│   ├── data/\n",
        "│   │   ├── file1.csv\n",
        "│   │   ├── file2.csv\n",
        "│   ├── metadata/\n",
        "│   │   ├── metadata_file1.json\n",
        "│   ├── calibration/\n",
        "│   │   ├── calibration_file1.cal\n",
        "│   ├── output/\n",
        "├── NRS01B/\n",
        "│   ├── data/\n",
        "│   │   ├── file3.csv\n",
        "│   │   ├── file4.csv\n",
        "│   ├── metadata/\n",
        "│   │   ├── metadata_file2.json\n",
        "│   ├── calibration/\n",
        "│   │   ├── calibration_file2.cal\n",
        "│   ├── output/\n",
        "```\n",
        "\n",
        "#### 2. User Input\n",
        "When prompted, enter the full path of the main directory. Example:\n",
        "```\n",
        "/content/drive/My Drive/MantaConvert/NRS01\n",
        "```\n",
        "\n",
        "#### 3. How It Works\n",
        "1. The script automatically locates subdirectories (e.g., `NRS01A`, `NRS01B`).\n",
        "2. Processes all CSV files in the `Data` folder of each subdirectory.\n",
        "3. Uses the corresponding metadata, calibration, and the shared `settings.json` file.\n",
        "4. Saves the output in the `output` folder within the respective subdirectory.\n",
        "\n",
        "#### 4. Key Points\n",
        "- **Ensure the `settings file` folder exists** in the main directory with `settings.json`.\n",
        "- Subdirectories must have `Data`, `metadata`, and `calibration` folders.\n",
        "- The script creates `output` folders automatically if not present.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGRhvxmay7Lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62d6f69-a650-480c-da85-a4446e3f1d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path directory in the base directory you want to convert (e.g., /content/drive/My Drive/MantaConvert/NRS01):  /content/drive/My Drive/MantaConvert/NRS01\n",
            "Found settings file: /content/drive/My Drive/MantaConvert/NRS01/settings file/nrs-hmd.json\n",
            "\n",
            "Processing folder: NRS01B\n",
            "Files found in NRS01B:\n",
            "Data files: 2 files found\n",
            "Metadata file: Found\n",
            "Calibration file: Found\n",
            "Settings file: Found\n",
            "Data files: /content/drive/My Drive/MantaConvert/NRS01/NRS01B/data/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.csv, /content/drive/My Drive/MantaConvert/NRS01/NRS01B/data/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.csv\n",
            "Metadata file: /content/drive/My Drive/MantaConvert/NRS01/NRS01B/metadata/NRS_01_20200915-20220925_HMD_v3-metadata.json\n",
            "Calibration file: /content/drive/My Drive/MantaConvert/NRS01/NRS01B/calibration/NRS01_2022_MANTA_Metadata_v3.xlsx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Ask the user for the base directory path\n",
        "base_dir = input(\"Enter the path directory in the base directory you want to convert (e.g., /content/drive/My Drive/MantaConvert/NRS01): \").strip()\n",
        "\n",
        "def list_subfolders(folder_path):\n",
        "    \"\"\"Lists all subfolders within a given folder path, sorted alphabetically.\"\"\"\n",
        "    try:\n",
        "        subfolders = sorted([f.name for f in os.scandir(folder_path) if f.is_dir()])\n",
        "        return subfolders\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing subfolders: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def list_files_in_folder(folder_path):\n",
        "    \"\"\"Lists all files within a given folder path, sorted alphabetically.\"\"\"\n",
        "    try:\n",
        "        file_list = sorted([file for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))])\n",
        "        return file_list\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def get_folder_files(subfolder_path, settings_file_path):\n",
        "    \"\"\"Get all required files for a specific subfolder.\"\"\"\n",
        "    files = {\n",
        "        'data': [],\n",
        "        'metadata_file': None,\n",
        "        'calibration_file': None,\n",
        "        'settings_file': settings_file_path  # Use the settings file from the 'settings file' folder\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get Data files\n",
        "        data_path = os.path.join(subfolder_path, 'data')\n",
        "        if os.path.exists(data_path):\n",
        "            files['data'] = [os.path.join(data_path, f) for f in list_files_in_folder(data_path) if f.lower().endswith('.csv')]\n",
        "\n",
        "        # Get metadata file\n",
        "        metadata_path = os.path.join(subfolder_path, 'metadata')\n",
        "        if os.path.exists(metadata_path):\n",
        "            metadata_files = list_files_in_folder(metadata_path)\n",
        "            if metadata_files:\n",
        "                files['metadata_file'] = os.path.join(metadata_path, metadata_files[0])\n",
        "\n",
        "        # Get calibration file\n",
        "        cal_path = os.path.join(subfolder_path, 'calibration')\n",
        "        if os.path.exists(cal_path):\n",
        "            cal_files = list_files_in_folder(cal_path)\n",
        "            if cal_files:\n",
        "                files['calibration_file'] = os.path.join(cal_path, cal_files[0])\n",
        "\n",
        "        return files\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting folder files: {str(e)}\")\n",
        "        return files\n",
        "\n",
        "# Ensure base directory exists\n",
        "if not os.path.exists(base_dir):\n",
        "    print(f\"Base directory {base_dir} does not exist!\")\n",
        "    exit()\n",
        "\n",
        "# Check if the 'settings file' folder exists in the base directory\n",
        "settings_folder = os.path.join(base_dir, 'settings file')\n",
        "if not os.path.exists(settings_folder):\n",
        "    print(\"Settings file folder not found in the base directory!\")\n",
        "    exit()\n",
        "\n",
        "# Look for the JSON settings file in the 'settings file' folder\n",
        "setting_files = [f for f in list_files_in_folder(settings_folder) if f.lower().endswith('.json')]\n",
        "if not setting_files:\n",
        "    print(\"No JSON settings file found in the 'settings file' folder. Please ensure it exists.\")\n",
        "    exit()\n",
        "\n",
        "# Assuming there's only one JSON settings file\n",
        "settings_file_path = os.path.join(settings_folder, setting_files[0])\n",
        "print(f\"Found settings file: {settings_file_path}\")\n",
        "\n",
        "# Get list of main subfolders (excluding 'settings file')\n",
        "subfolders = [folder for folder in list_subfolders(base_dir) if folder != \"settings file\"]\n",
        "\n",
        "# Process each subfolder\n",
        "folder_files = {}\n",
        "for folder in subfolders:\n",
        "    try:\n",
        "        print(f\"\\nProcessing folder: {folder}\")\n",
        "        folder_path = os.path.join(base_dir, folder)\n",
        "        folder_files[folder] = get_folder_files(folder_path, settings_file_path)\n",
        "        folder_files[folder]['folder_path'] = folder_path\n",
        "        folder_files[folder]['output_path'] = os.path.join(folder_path, 'Output')\n",
        "\n",
        "        print(f\"Files found in {folder}:\")\n",
        "        print(f\"Data files: {len(folder_files[folder]['data'])} files found\")\n",
        "        print(f\"Metadata file: {'Found' if folder_files[folder]['metadata_file'] else 'Not found'}\")\n",
        "        print(f\"Calibration file: {'Found' if folder_files[folder]['calibration_file'] else 'Not found'}\")\n",
        "        print(f\"Settings file: {'Found' if folder_files[folder]['settings_file'] else 'Not found'}\")\n",
        "        if folder_files[folder]['data']:\n",
        "            print(f\"Data files: {', '.join(folder_files[folder]['data'])}\")\n",
        "        if folder_files[folder]['metadata_file']:\n",
        "            print(f\"Metadata file: {folder_files[folder]['metadata_file']}\")\n",
        "        if folder_files[folder]['calibration_file']:\n",
        "            print(f\"Calibration file: {folder_files[folder]['calibration_file']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing folder {folder}: {str(e)}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qys3P7zGiME"
      },
      "source": [
        "# `BaseConverter` Class\n",
        "\n",
        "The `BaseConverter` class is designed to handle the conversion of CSV files to **netCDF PAD** format, managing settings, metadata, logging, and system configurations.\n",
        "\n",
        "### Key Methods\n",
        "\n",
        "1. **`__init__()`**:\n",
        "   - Initializes attributes for settings, metadata, and manifest.\n",
        "   - Reads the `settings.json` and `system.ini` files to configure the environment.\n",
        "   - Sets up logging in the `logBasePath` directory.\n",
        "\n",
        "2. **`setup_logging()`**:\n",
        "   - Configures logging to record process details in `app.log`.\n",
        "   - Supports both file-based and console-based logging.\n",
        "\n",
        "3. **`get_settings_path()`**:\n",
        "   - Locates and returns the path to the `settings.json` file used for conversion parameters.\n",
        "\n",
        "4. **`get_metadata(metadata_file)`**:\n",
        "   - Reads and stores metadata from a file into the `metadata` attribute.\n",
        "   - Adds DOI information to the metadata.\n",
        "\n",
        "5. **`parse_manifest(manifest_path, backup=True)`**:\n",
        "   - Parses a manifest file into a dictionary linking file paths to checksums.\n",
        "   - Creates a backup of the manifest file if specified.\n",
        "\n",
        "6. **`mint_doi()`**:\n",
        "   - Placeholder for minting dataset-level DOIs, logging this as a critical message.\n",
        "\n",
        "### Workflow of Files\n",
        "- **Settings File** (`settings.json`): Contains citation, DOI, and configuration parameters.\n",
        "- **System Configuration File** (`system.ini`): Defines paths and other system-wide settings.\n",
        "- **Metadata File**: Provides detailed dataset information, enriched with DOI data.\n",
        "- **Manifest File**: Links file paths to checksums for validation.\n",
        "- **Log Files**: Records process activities, errors, and warnings in the `logBasePath` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CFejMxlAjge"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class BaseConverter(object):\n",
        "    \"\"\"Base class for csv file to netCDF PAD converters. This base class\n",
        "    handles the request and parsing of a settings file and system.ini\n",
        "    file.\n",
        "    \"\"\"\n",
        "    def __init__(self, get_settings=True):\n",
        "\n",
        "        # Instantiate attributes.\n",
        "        self.package_path = None\n",
        "        self.metadata = None\n",
        "        self.manifest = None\n",
        "\n",
        "        # Get settings file to use and grab DOI URL from file (may be Null if\n",
        "        # we'll be minting DOIs per dataset).\n",
        "        if get_settings:\n",
        "            # settings_file = self.get_settings_path()\n",
        "            with open(settings_file) as f:\n",
        "                settings = json.load(f)\n",
        "\n",
        "            self.citation = settings['citationText']\n",
        "            self.doi = settings['doi']\n",
        "            self.doi_minting = settings['mintDOI']\n",
        "            self.version = settings['version']\n",
        "\n",
        "            if 'acknowledgement' in settings:\n",
        "                self.acknowledgement = settings['acknowledgement']\n",
        "\n",
        "        # Get root directory of CPI system.\n",
        "        cwd = os.getcwd()\n",
        "        cpi_root = os.path.join(cwd.split('/CPI')[0], 'CPI')\n",
        "\n",
        "        # Read system.ini file\n",
        "        system_file = os.path.join(cpi_root, 'system.ini')\n",
        "        system = ConfigParser()\n",
        "        system.read(system_file)\n",
        "\n",
        "        # Setting paths\n",
        "        self.starting_path = os.getcwd()\n",
        "        self.log_base = os.path.join(os.getcwd(), 'logBasePath')\n",
        "        self.log_level = os.path.join(os.getcwd(), 'logLevel')\n",
        "\n",
        "        # Check if the log directory exists, if not, create it\n",
        "        if not os.path.exists(self.log_base):\n",
        "            os.makedirs(self.log_base)  # Create the directory if it doesn't exist\n",
        "            print(f\"Created log base directory: {self.log_base}\")\n",
        "\n",
        "        # Check if the log level file exists, create it if it doesn't\n",
        "        if not os.path.exists(self.log_level):\n",
        "            with open(self.log_level, 'w') as f:\n",
        "                f.write('INFO')  # You can change the log level to your preference\n",
        "            print(f\"Created log level file: {self.log_level}\")\n",
        "\n",
        "        # Setup logging configuration\n",
        "        self.setup_logging()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging to write to a file in the log base path.\"\"\"\n",
        "        log_level = logging.INFO  # Default to INFO if file is not available\n",
        "\n",
        "        # Check the log level from the file, default to INFO if file not found or invalid content\n",
        "        if os.path.exists(self.log_level):\n",
        "            with open(self.log_level, 'r') as f:\n",
        "                level = f.read().strip()\n",
        "                # Map the string level to logging constants, default to INFO if invalid\n",
        "                log_level = getattr(logging, level, logging.INFO)\n",
        "\n",
        "        log.setLevel(log_level)\n",
        "\n",
        "        # Set log file path\n",
        "        log_file_path = os.path.join(self.log_base, 'app.log')\n",
        "\n",
        "        # Create file handler for logging\n",
        "        file_handler = logging.FileHandler(log_file_path)\n",
        "        file_handler.setLevel(log_level)\n",
        "\n",
        "        # Create formatter for log messages\n",
        "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "        file_handler.setFormatter(formatter)\n",
        "\n",
        "        # Add file handler to logger\n",
        "        log.addHandler(file_handler)\n",
        "\n",
        "        # You can also add a stream handler for console output if needed\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setLevel(log_level)\n",
        "        stream_handler.setFormatter(formatter)\n",
        "        log.addHandler(stream_handler)\n",
        "\n",
        "\n",
        "        print(f\"Logging is set up. Logs will be written to {log_file_path}\")\n",
        "\n",
        "\n",
        "    def get_settings_path(self):\n",
        "        \"\"\"Get path to settings file\n",
        "\n",
        "        The settings file is a json file containing control parameters.\n",
        "        For netCDF conversion this is used to pass in existing DOI\n",
        "        information only despite the settings file (also used for\n",
        "        pre-ingest)  contains many more values.\n",
        "\n",
        "        This method asks the user for the name of the settings file to use.\n",
        "        Settings files are located in a folder named cpi_settings/pad in the\n",
        "        user's home directory. The important key/value pairs for this method is\n",
        "        \"doi\": \"https://doi.org/10.25921/saca-sp25\" \"citation\": \"Citation text\".\n",
        "\n",
        "        Returns:\n",
        "            settings_path(str): path to the settings json file.\n",
        "\n",
        "        \"\"\"\n",
        "        # Get list of settings files in user home directory\n",
        "        root_path = os.path.join(os.path.expanduser(\"~\"),\n",
        "                                 'cpi_settings/pad')\n",
        "        root, dirs, files = self.walk_dir(root_path)\n",
        "        files.sort()\n",
        "        print(f'\\nSettings files in {root}:  {files}')\n",
        "        # settings_file = input(\"Enter name of settings file to use: \")\n",
        "        settings_path = 'settings.json'\n",
        "        # if not os.path.splitext(settings_file)[1]:\n",
        "        #     settings_file = settings_file + '.json'\n",
        "        # settings_path = os.path.join(root, settings_file)\n",
        "        if os.path.isfile(settings_path):\n",
        "            return settings_path\n",
        "        else:\n",
        "            print(f'{settings_path} is not a valid file path. '\n",
        "                  f'Enter name of the file again: ')\n",
        "            return self.get_settings_path()\n",
        "\n",
        "    def get_metadata(self, metadata_file):\n",
        "        \"\"\"Open metadata file and convert to Python dictionary held in\n",
        "        self.metadata attribute.\n",
        "\n",
        "        Args:\n",
        "            metadata_file (str): Full path to metadata file\n",
        "\n",
        "        \"\"\"\n",
        "        with open(metadata_file, 'r') as f:\n",
        "            self.metadata = json.loads(f.read())\n",
        "\n",
        "        self.metadata['DOI'] = self.doi\n",
        "\n",
        "    def parse_manifest(self, manifest_path, backup=True):\n",
        "        \"\"\"Parse manifest file and convert into a dictionary with file path and\n",
        "        key and checksum as value and store in attribute self.manifest.\n",
        "\n",
        "        Args:\n",
        "            manifest_path (str): Full path to manifest file.\n",
        "            backup (boo): Control whether to make copy of manifest file before\n",
        "                          parsing.\n",
        "\n",
        "        \"\"\"\n",
        "        if backup:\n",
        "            # Make a backup of the original manifest before continuing. If a\n",
        "            # backup already exists skip overwriting to preserve original\n",
        "            # manifest when doing multiple processing runs.\n",
        "            path_parts = os.path.split(manifest_path)\n",
        "            backup_path = os.path.join(path_parts[0],\n",
        "                                       f'original-{path_parts[1]}')\n",
        "            if not os.path.isfile(backup_path):\n",
        "                shutil.copy2(manifest_path, backup_path)\n",
        "\n",
        "        manifest = {}\n",
        "        with open(manifest_path) as m:\n",
        "            for line in m:\n",
        "                entry = line.split('  ')\n",
        "                # print(entry[1][:-1], entry[0])\n",
        "                manifest[entry[1][:-1]] = entry[0]\n",
        "\n",
        "        self.manifest = manifest\n",
        "\n",
        "    def mint_doi(self):\n",
        "        \"\"\"Mint DOI. This is a placeholder for eventual development.\n",
        "\n",
        "        \"\"\"\n",
        "        print(f'Code can not yet mint a dataset;level DOI')\n",
        "        log.critical(f'Code can not yet mint a dataset-level DOI')\n",
        "        self.metadata['DOI'] = ''\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl8jZPW0H9aT"
      },
      "source": [
        "# `MantaConvert` Class\n",
        "\n",
        "The `MantaConvert` class is responsible for converting **MANTA output CSV files** into standardized **netCDF files**. It processes raw data, applies quality checks, incorporates metadata and calibration data, and produces netCDF files suitable for environmental and acoustic analysis.\n",
        "\n",
        "### Methods Explanation\n",
        "\n",
        "#### **`__init__()`**\n",
        "- Initializes the class with input file paths, metadata, calibration data, and version.\n",
        "- Processes the CSV file to extract timestamps, frequencies, efforts, and PSD data.\n",
        "- Calls `create_acoustic_nc()` to generate the netCDF file.\n",
        "\n",
        "#### **`process_csv()`**\n",
        "- Reads the CSV file into a Pandas DataFrame.\n",
        "- Cleans the data by:\n",
        "  - Removing duplicate rows.\n",
        "  - Handling rows with duplicate timestamps and invalid efforts (e.g., 1-second errors).\n",
        "- Returns a cleaned DataFrame for further processing.\n",
        "\n",
        "#### **`create_acoustic_nc()`**\n",
        "- Generates a netCDF file from the processed data.\n",
        "- Incorporates timestamps, frequencies, PSD data, and metadata.\n",
        "- Adds quality flags and calibration data.\n",
        "- Handles file versioning and saves the output netCDF file.\n",
        "\n",
        "#### **`get_times()`**\n",
        "- Converts timestamps into numerical values (e.g., seconds since a reference time) for inclusion in netCDF files.\n",
        "\n",
        "#### **`get_dates()`**\n",
        "- Converts numerical time values back into readable ISO timestamps.\n",
        "\n",
        "#### **`time_variable_attrs()`**\n",
        "- Defines metadata attributes for time variables, such as standard names, units, and descriptions.\n",
        "\n",
        "#### **`static_metadata()`**\n",
        "- Returns static metadata, such as keywords, references, and processing details, to include in the netCDF file.\n",
        "\n",
        "#### **`get_metadata()`**\n",
        "- Updates and integrates metadata into the netCDF file.\n",
        "- Includes information about the project, dataset, quality flags, and other attributes.\n",
        "\n",
        "#### **`build_quality()`**\n",
        "- Initializes a `quality_flag` array to track data quality across time-frequency bins.\n",
        "\n",
        "#### **`get_quality()`**\n",
        "- Parses raw quality data and converts it into a structured format suitable for the netCDF file.\n",
        "\n",
        "#### **`get_manta_netcdf()`**\n",
        "- Attempts to retrieve existing MANTA netCDF export files for additional metadata and calibration data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9B6gLr7Ajge"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\"\"\"\n",
        "Converter for generating data netCDF files from MANTA output .csv files.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "log = logging.getLogger('MANTA.Converter')\n",
        "\n",
        "\n",
        "class MantaConvert():\n",
        "\n",
        "    def __init__(self, file_path, metadata, cal_data, version):\n",
        "        super(MantaConvert, self).__init__()\n",
        "\n",
        "        self.converter_version = 'v.1.2.0'\n",
        "        self.ds = None\n",
        "        self.out_file = None\n",
        "        self.manta_nc = None\n",
        "        self.metadata = metadata\n",
        "        self.file_path = file_path\n",
        "        self.cal_data = cal_data\n",
        "        self.version = version\n",
        "        self.old_file = None\n",
        "\n",
        "\n",
        "        # Check that the data file exists.\n",
        "        # if not os.path.isfile(file_path):\n",
        "        #     raise IOError(f'File {file_path} is not in data package.')\n",
        "\n",
        "        clean_data = self.process_csv()\n",
        "\n",
        "        # Get timestamps from clean_data DataFrame.\n",
        "        raw_time_stamps = clean_data['Timestamp']\n",
        "        self.time_stamps = np.array([stamp.to_pydatetime() for stamp in raw_time_stamps[1:]])\n",
        "        raw_freqs = clean_data.columns.values.tolist()\n",
        "        self.frequencies = np.array([float(x) for x in raw_freqs[2:]])\n",
        "\n",
        "\n",
        "        time_stamps = np.array([stamp.to_pydatetime() for stamp in\n",
        "                                raw_time_stamps[1:]])\n",
        "        # Get frequencies from DataFrame column values.\n",
        "\n",
        "        frequencies = np.array([float(x) for x in raw_freqs[2:]])\n",
        "\n",
        "        # Get the effort seconds for the data rows.\n",
        "        efforts = clean_data['0'].values[1:]\n",
        "        self.efforts = efforts\n",
        "\n",
        "        # Get main data array from DataFrame.\n",
        "        psd = clean_data.iloc[1:, 2:].to_numpy()\n",
        "        self.psd = psd\n",
        "\n",
        "        # Create nc file.\n",
        "        self.create_acoustic_nc(time_stamps, frequencies, efforts, psd,metadata, file_path)\n",
        "\n",
        "    def process_csv(self):\n",
        "        \"\"\"Read CSV file into a Pandas DataFrame and then clean data of\n",
        "        duplicate rows and rows with a duplicate timestamp but with an effort\n",
        "        of 1 second. These rows are a known errant output from MANTA.\n",
        "\n",
        "        Returns:\n",
        "            clean_data (pd.DataFrame): Cleaned DataFrame of .csv file data.\n",
        "            None - returned when there are duplicate rows that both appear to\n",
        "            have real data.\n",
        "        \"\"\"\n",
        "        parse_dates = {\"Timestamp\": [0]}\n",
        "        # Convert csv file to Pandas DataFrame\n",
        "        raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Remove rows that are complete duplicates and log removal.\n",
        "        no_dups = raw_data.drop_duplicates()\n",
        "        if raw_data.shape != no_dups.shape:\n",
        "            removed_num = raw_data.shape[0] - no_dups.shape[0]\n",
        "            log.info(f'{removed_num} duplicate rows were removed.from'\n",
        "                     f' {os.path.basename(self.file_path)}')\n",
        "\n",
        "        # Sort by Timestamp and effort seconds (column \"0\" which is the column\n",
        "        # name not its index)) and check for duplicate timestamp values.\n",
        "        no_dups = no_dups.sort_values(by=[\"Timestamp\", \"0\"])\n",
        "        time_diffs = no_dups['Timestamp'].diff()\n",
        "        drop_rows = []\n",
        "        for index, value in enumerate(time_diffs):\n",
        "            value = value.total_seconds()\n",
        "            first_effort = no_dups.iloc[index - 1][1]\n",
        "            second_effort = no_dups.iloc[index][1]\n",
        "            if value == 0.0:\n",
        "                # We have found a set of duplicate timestamps. THere is a MANTA\n",
        "                # issue where a row with an effort of 1 second is followed by a\n",
        "                # row with a 60-second effort. The 1-second row is an error and\n",
        "                # needs to be dropped. If the effort is not 1, then there is a\n",
        "                # deeper issue so throw an error.\n",
        "                if first_effort == 1:\n",
        "                    # This is an error and this row should be dropped.\n",
        "                    drop_rows.append(index-1)\n",
        "                else:\n",
        "                    log.error(f'{os.path.basename(self.file_path)} has '\n",
        "                              f'duplicate timestamps with differing data '\n",
        "                              f'values and efforts over 1 second. '\n",
        "                              f'Skipping file.')\n",
        "                    raise ValueError\n",
        "            elif value == 59.0:\n",
        "                # This might be from a spurious 1-second effort row in\n",
        "                # addition to  full 60-second effort row for the same minute.\n",
        "                # Test for this and delete row if appropriate.\n",
        "                if second_effort == 1 and first_effort == 60:\n",
        "                    drop_rows.append(index)\n",
        "\n",
        "        if drop_rows:\n",
        "            log.info(f'The following row(s) qre 1-second errors and were '\n",
        "                     f'removed {drop_rows} from '\n",
        "                     f'{os.path.basename(self.file_path)}')\n",
        "            clean_data = no_dups.drop(drop_rows, axis=0)\n",
        "        else:\n",
        "            clean_data = no_dups\n",
        "\n",
        "        return clean_data\n",
        "\n",
        "    @staticmethod\n",
        "    def get_times(time_stamps, units='seconds since 1970-01-01T00:00:00Z'):\n",
        "        \"\"\"Get times from timestamps.\n",
        "\n",
        "        Args:\n",
        "            time_stamps (numpy array): Time stamp array.\n",
        "            units (str): units for time conversion. seconds by default.\n",
        "\n",
        "        Returns:\n",
        "            cmd (numpy array): Times array.\n",
        "        \"\"\"\n",
        "\n",
        "        cdm_time = []\n",
        "        for stamp in time_stamps:\n",
        "            temp_stamp = date2num(stamp, units=units)\n",
        "            cdm_time.append(temp_stamp)\n",
        "        return np.array(cdm_time)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dates(times, units='seconds since 1970-01-01T00:00:00Z'):\n",
        "        \"\"\"Get times from timestamps.\n",
        "\n",
        "        Args:\n",
        "            times (list): list of integer seconds since timestamps\n",
        "            units (str): units for time conversion. seconds by default.\n",
        "\n",
        "        Returns:\n",
        "            dates (list): list of times converted to ISO timestamps.\n",
        "        \"\"\"\n",
        "\n",
        "        dates = []\n",
        "        for stamp in times:\n",
        "            temp_stamp = num2date(stamp, units=units)\n",
        "            dates.append(temp_stamp)\n",
        "\n",
        "        return dates\n",
        "\n",
        "    def get_manta_netcdf(self):\n",
        "        \"\"\"Get the MANTA netCDF_converters export file tor the data file\n",
        "        being processed.\n",
        "\n",
        "        Returns:\n",
        "            nc (netCDF_converters): Open netCDF_converters file.\n",
        "        \"\"\"\n",
        "        # Due to MANTA differences in file organization and data packaging\n",
        "        # there are several possible paths. We need to try them all to find\n",
        "        # the correct path.\n",
        "        path_1 = self.out_file.replace('.nc', '_netCDF.nc')\n",
        "        path_2 = path_1.replace('csv', 'netCDF')\n",
        "        path_3 = self.out_file.replace(f'_v{self.version}.nc',\n",
        "                                       f'_netCDF_v{self.version}.nc')\n",
        "        path_4 = path_3.replace('csv', 'netCDF')\n",
        "\n",
        "\n",
        "        paths = [path_1, path_2, path_3, path_4]\n",
        "        for file_path in paths:\n",
        "            if os.path.isfile(file_path):\n",
        "                with xr.open_dataset(file_path) as manta_ds:\n",
        "                    return manta_ds\n",
        "\n",
        "    @staticmethod\n",
        "    def time_variable_attrs(time_stamps, cdm_time):\n",
        "        \"\"\"Create time variables.\n",
        "\n",
        "        Args:\n",
        "            time_stamps (array): String timestamp values.\n",
        "            cdm_time (array):\n",
        "\n",
        "        Returns:\n",
        "            time_stamp_var: Formatted time stamp variable\n",
        "            time_var: Formatted time variable.\n",
        "\n",
        "        \"\"\"\n",
        "        time_stamp_var = {\n",
        "            'actual_range': (time_stamps[0].isoformat(),\n",
        "                             time_stamps[-1].isoformat()),\n",
        "            # 'comment': 'Start times of 1-minute bins over which '\n",
        "            #            'sound pressure levels are calculated',\n",
        "            'long_name': \"ISO timestamp beginning each 1-minute temporal \"\n",
        "                         \"bin\",\n",
        "            'standard_name': \"time\",\n",
        "            'units': 'seconds'\n",
        "        }\n",
        "\n",
        "        time_var = {\n",
        "            'actual_range': (cdm_time[0], cdm_time[-1]),\n",
        "            'long_name': \"Time beginning each 1-minute temporal bin\",\n",
        "            'standard_name': \"time\",\n",
        "            'units': \"seconds since 1970-01-01T00:00:00Z\",\n",
        "            'coverage_content_type': 'coordinate'\n",
        "        }\n",
        "\n",
        "        return time_stamp_var, time_var\n",
        "\n",
        "    @staticmethod\n",
        "    def static_metadata(md):\n",
        "        \"\"\"Static metadata information be be added ro metadata dictionary.\n",
        "\n",
        "        Returns:\n",
        "            static_metadata (dict): static metadata dictionary.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            static_metadata = {\n",
        "                'keywords': 'GCMD:oceans, GCMD:ocean acoustics, GCMD:ambient noise,'\n",
        "                            ' intensity, GCMD:marine environment monitoring, '\n",
        "                            'marine habitat, sound intensity level in water, '\n",
        "                            'soundscapes',\n",
        "                'keyVocabulary': \"GCMD: GCMD Keywords\",\n",
        "                'source': \"Data analysis was performed using the Making Ambient \"\n",
        "                        \"Noise Trends Accessible (MANTA, https://bitbucket.org\"\n",
        "                        \"/CLO-BRP/manta-wiki/wiki/Home, see Miksis-Olds et al., \"\n",
        "                        \"2021; Martin et al., 2021a,b) standalone software \"\n",
        "                        f\"({md['DATASET_DETAILS']['SOFTWARE_VERSION']}) \"\n",
        "                        f\"to produce hybrid millidecade spectra of \"\n",
        "                        \"sound levels from ocean audio recordings. To \"\n",
        "                        \"efficiently tackle large datasets, MANTA is designed \"\n",
        "                        \"around a parallel-processing Matlab package, Raven-X \"\n",
        "                        \"(Dugan et. al., 2014, 2016, and 2018) that uses \"\n",
        "                        \"ordinary multi-core computers to accelerate processing \"\n",
        "                        \"speeds. MANTA calculates the sound pressure spectral \"\n",
        "                        \"density (PSD) levels in units of 1 µPa^2/Hz using \"\n",
        "                        \"Welch's Method in Matlab. The Discrete Fourier \"\n",
        "                        \"Transform length is equal to the sample rate, a Hann \"\n",
        "                        \"window of equal length is applied to the data and 50% \"\n",
        "                        \"overlap is used. This results in PSD estimates of \"\n",
        "                        \"mean-square pressure amplitude (µPa^2) with a frequency \"\n",
        "                        \"resolution of 1 Hz and temporal resolution of 1 second. \"\n",
        "                        \"The 120 PSD estimates from each 1-minute segment were \"\n",
        "                        \"averaged, and the average spectrum for each minute was \"\n",
        "                        \"further processed to a hybrid millidecade (HMD) \"\n",
        "                        \"spectrum as dB re 1 µPa^2/Hz, as defined in Martin et \"\n",
        "                        \"al. (2021b). Hybrid millidecades are an efficient means \"\n",
        "                        \"of storing PSD spectra from high sample rate audio \"\n",
        "                        \"files using 1-Hz values up to 435 Hz, then millidecade \"\n",
        "                        \"wide PSD values up to one half of the sampling rate \"\n",
        "                        \"(Martin et al., 2021b). The MANTA outputs for each day \"\n",
        "                        \"are: (1) CSV of the 1 minute HMD results; (2) image of \"\n",
        "                        \"the daily long-term spectral average based on the \"\n",
        "                        \"1 minute HMD results, (3) image of the daily spectral \"\n",
        "                        \"probability density with percentiles, and (4) NetCDF \"\n",
        "                        \"containing products 2 and 3 in addition to a deployment-\"\n",
        "                        \"level MANTA Metadata output file containing the \"\n",
        "                        \"associated frequency-dependent calibration data used to \"\n",
        "                        \"compute the calibrated spectrum levels.\",\n",
        "                'references':  'Original audio recordings are available '\n",
        "                            'open-access: '\n",
        "                            'https://www.ncei.noaa.gov/maps'\n",
        "                            '/passive-acoustic-data/. Computation of '\n",
        "                            'single-sided mean-square sound pressure '\n",
        "                            'spectral density with 1 Hz resolution followed '\n",
        "                            'ISO 18405 3.1.3.13 (International Standard ISO 1'\n",
        "                            '8405:2017(E), Underwater Acoustics – Terminology. '\n",
        "                            'Geneva: ISO). Hybrid millidecade band processing '\n",
        "                            'followed Martin et al. (2021; '\n",
        "                            'https://doi.org/10.1121/10.0003324)',\n",
        "                'license': 'CC0-1.0',\n",
        "            }\n",
        "        except:\n",
        "            static_metadata = {\n",
        "                'keywords': 'GCMD:oceans, GCMD:ocean acoustics, GCMD:ambient noise,'\n",
        "                            ' intensity, GCMD:marine environment monitoring, '\n",
        "                            'marine habitat, sound intensity level in water, '\n",
        "                            'soundscapes',\n",
        "                'keyVocabulary': \"GCMD: GCMD Keywords\",\n",
        "                'source': \"Data analysis was performed using the Making Ambient \"\n",
        "                        \"Noise Trends Accessible (MANTA, https://bitbucket.org\"\n",
        "                        \"/CLO-BRP/manta-wiki/wiki/Home, see Miksis-Olds et al., \"\n",
        "                        \"2021; Martin et al., 2021a,b) standalone software \"\n",
        "                        \"(Unknown Version)\"\n",
        "                        \"to produce hybrid millidecade spectra of \"\n",
        "                        \"sound levels from ocean audio recordings. To \"\n",
        "                        \"efficiently tackle large datasets, MANTA is designed \"\n",
        "                        \"around a parallel-processing Matlab package, Raven-X \"\n",
        "                        \"(Dugan et. al., 2014, 2016, and 2018) that uses \"\n",
        "                        \"ordinary multi-core computers to accelerate processing \"\n",
        "                        \"speeds. MANTA calculates the sound pressure spectral \"\n",
        "                        \"density (PSD) levels in units of 1 µPa^2/Hz using \"\n",
        "                        \"Welch's Method in Matlab. The Discrete Fourier \"\n",
        "                        \"Transform length is equal to the sample rate, a Hann \"\n",
        "                        \"window of equal length is applied to the data and 50% \"\n",
        "                        \"overlap is used. This results in PSD estimates of \"\n",
        "                        \"mean-square pressure amplitude (µPa^2) with a frequency \"\n",
        "                        \"resolution of 1 Hz and temporal resolution of 1 second. \"\n",
        "                        \"The 120 PSD estimates from each 1-minute segment were \"\n",
        "                        \"averaged, and the average spectrum for each minute was \"\n",
        "                        \"further processed to a hybrid millidecade (HMD) \"\n",
        "                        \"spectrum as dB re 1 µPa^2/Hz, as defined in Martin et \"\n",
        "                        \"al. (2021b). Hybrid millidecades are an efficient means \"\n",
        "                        \"of storing PSD spectra from high sample rate audio \"\n",
        "                        \"files using 1-Hz values up to 435 Hz, then millidecade \"\n",
        "                        \"wide PSD values up to one half of the sampling rate \"\n",
        "                        \"(Martin et al., 2021b). The MANTA outputs for each day \"\n",
        "                        \"are: (1) CSV of the 1 minute HMD results; (2) image of \"\n",
        "                        \"the daily long-term spectral average based on the \"\n",
        "                        \"1 minute HMD results, (3) image of the daily spectral \"\n",
        "                        \"probability density with percentiles, and (4) NetCDF \"\n",
        "                        \"containing products 2 and 3 in addition to a deployment-\"\n",
        "                        \"level MANTA Metadata output file containing the \"\n",
        "                        \"associated frequency-dependent calibration data used to \"\n",
        "                        \"compute the calibrated spectrum levels.\",\n",
        "                'references':  'Original audio recordings are available '\n",
        "                            'open-access: '\n",
        "                            'https://www.ncei.noaa.gov/maps'\n",
        "                            '/passive-acoustic-data/. Computation of '\n",
        "                            'single-sided mean-square sound pressure '\n",
        "                            'spectral density with 1 Hz resolution followed '\n",
        "                            'ISO 18405 3.1.3.13 (International Standard ISO 1'\n",
        "                            '8405:2017(E), Underwater Acoustics – Terminology. '\n",
        "                            'Geneva: ISO). Hybrid millidecade band processing '\n",
        "                            'followed Martin et al. (2021; '\n",
        "                            'https://doi.org/10.1121/10.0003324)',\n",
        "                'license': 'CC0-1.0',\n",
        "            }\n",
        "\n",
        "\n",
        "        return static_metadata\n",
        "\n",
        "    def get_metadata(self, md):\n",
        "        \"\"\"Update global netCDF_converters metadata information.\n",
        "\n",
        "        Args:\n",
        "            md (dict): Dictionary containing dataset metadata.\n",
        "\n",
        "        \"\"\"\n",
        "        creation_date = date.today().isoformat()\n",
        "        md.update(self.static_metadata(md))\n",
        "        data_quality, quality_bins = self.get_quality(md)\n",
        "\n",
        "        # Process raw scientists, projects and sponsors.\n",
        "        scientists = []\n",
        "        for entry in md['SCIENTISTS']:\n",
        "            if 'name' in entry:\n",
        "                scientists.append(entry['name'])\n",
        "        scientists = ', '.join(scientists)\n",
        "\n",
        "        project = ', '.join(md['PROJECT_NAME'])\n",
        "        # Hack to fix ONMS_sound project name\n",
        "        if project == 'ONMS':\n",
        "            project = 'ONMS Sound'\n",
        "\n",
        "        sources = []\n",
        "        if 'SPONSORS' in md:\n",
        "            for entry in md['SPONSORS']:\n",
        "                if 'name' in entry:\n",
        "                    sources.append(entry['name'])\n",
        "        sources = ', '.join(sources)\n",
        "\n",
        "        raw_metadata = {\n",
        "            'acknowledgement': '',\n",
        "            'history': f'Original hybrid millidecade spectra were produced by '\n",
        "                       f'{scientists}. NCEI created this single '\n",
        "                       f'standards-compliant netCDF file from the MANTA '\n",
        "                       f'outputs plus additional metadata from the deployment '\n",
        "                       f'and overall project. Conversion was done using '\n",
        "                       f'{self.converter_version} of the NCEI MANTA netCDF '\n",
        "                       f'converter.',\n",
        "            'citation': f'Cite as: {md.get(\"citation\", \"\")}',\n",
        "            'comment': data_quality,\n",
        "            'creator_name': scientists,\n",
        "            'creator_role': \"Principal Investigator\",\n",
        "            'conventions': 'COARDS, CF-1.6, ACDD-1.3',\n",
        "            'publisher_email': 'pad.info@noaa.gov',\n",
        "            'publisher_name': ' NOAA National Centers for Environmental '\n",
        "                               'Information',\n",
        "            'publisher_type': 'institution',\n",
        "            'publisher_url': 'https://www.ncei.noaa.gov/products'\n",
        "                             '/passive-acoustic-data',\n",
        "            'date_created': creation_date,\n",
        "            'id': md.get('DOI', ''),\n",
        "            'product_version': f'v{md.get(\"version\", \"\")}',\n",
        "            'naming_authority': 'NOAA National Centers for Environmental '\n",
        "                                'Information',\n",
        "            'infoUrl': 'https://ncei.noaa.gov',\n",
        "            'institution': sources,\n",
        "            'geospatial_bounds': f\"POINT ({md.get('DEPLOYMENT', {}).get('DEPLOY_LAT', '')} \"\n",
        "                                 f\"{md.get('DEPLOYMENT', {}).get('DEPLOY_LON', '')})\",\n",
        "            'time_coverage_duration': \"P1D\",\n",
        "            'time_coverage_resolution': \"P60S\",\n",
        "            'keywords': md.get(\"keywords\", \"\"),\n",
        "            'keywords_vocabulary': md.get(\"keyVocabulary\", \"\"),\n",
        "            'license': md.get('license', ''),\n",
        "            'project': project,\n",
        "            'instrument': md.get('INSTRUMENT_TYPE', ''),\n",
        "            'standard_name_vocabulary': 'CF Standard Name Table v80',\n",
        "            'summary': md.get('ABSTRACT', ''),\n",
        "            'source': md.get('source', ''),\n",
        "            'title':  md.get('TITLE', ''),\n",
        "            'time_offset': f\"{md.get('DATASET_DETAILS', {}).get('ANALYSIS_TIME_ZONE', '0')} \"\n",
        "                           f\"hours from UTC\",\n",
        "            'reference': md.get('references', '')\n",
        "        }\n",
        "\n",
        "        # Populate acknowledgement field with value from settings file or\n",
        "        # remove key from raw_metadata if there is no acknowledgement\n",
        "        # information.\n",
        "        if 'acknowledgement' in md:\n",
        "            if md['acknowledgement']:\n",
        "                raw_metadata['acknowledgement'] = md['acknowledgement']\n",
        "            else:\n",
        "                raw_metadata.pop('acknowledgement')\n",
        "        else:\n",
        "            raw_metadata.pop('acknowledgement')\n",
        "\n",
        "\n",
        "        keys = sorted(raw_metadata.keys())\n",
        "        global_attrs = {key:raw_metadata[key] for key in keys}\n",
        "\n",
        "\n",
        "        return global_attrs, quality_bins\n",
        "\n",
        "    def get_quality(self, raw_metadata, format='%Y-%m-%dT%H:%M:%S'):\n",
        "\n",
        "        \"\"\"Convert raw data quality information into JSON string suitable for\n",
        "        map viewer display.\n",
        "\n",
        "        Args:\n",
        "            raw_metadata(dict): Dictionary of dataset-level metadata.\n",
        "            format (str): Timestamp strong format.\n",
        "\n",
        "        Returns(dict): parsed quality details.\n",
        "\n",
        "        \"\"\"\n",
        "        quality_list = []\n",
        "        quality_bins = []\n",
        "\n",
        "        quality_values = {'Good': 1,\n",
        "                          'Unverified': 2,\n",
        "                          'Compromised': 3,\n",
        "                          'Unusable': 4}\n",
        "\n",
        "        raw_quality = raw_metadata.get('QUALITY_DETAILS', {}).get('quality_details', [])\n",
        "        for index, entry in enumerate(raw_quality):\n",
        "            if entry['quality'] == 'Select Quality Level':\n",
        "                # This is a bogus entry from PassivePacker so set to nothing.\n",
        "                continue\n",
        "            start = entry[\"start\"]\n",
        "            end = entry[\"end\"]\n",
        "            low_freq = entry['low_freq']\n",
        "            high_freq = entry['high_freq']\n",
        "            if low_freq and high_freq:\n",
        "                freq_str = f'from {low_freq}Hz to {high_freq}Hz'\n",
        "            else:\n",
        "                freq_str = f''\n",
        "\n",
        "            raw_channels = entry['channels']\n",
        "            if raw_channels:\n",
        "                if len(raw_channels) == 1:\n",
        "                    chan_str = f'for channel {raw_channels}'\n",
        "                else:\n",
        "                    chan_str = f'for channels {raw_channels}'\n",
        "            else:\n",
        "                chan_str = ''\n",
        "\n",
        "            quality = (f'Data quality: {entry[\"quality\"]} {start} to {end}'\n",
        "                       f' {chan_str} '\n",
        "                       f'{freq_str}')\n",
        "\n",
        "            details = ''\n",
        "            if 'comments' in entry:\n",
        "                details = entry['comments']\n",
        "            if details:\n",
        "                if len(details) > 4:\n",
        "                    # Check length of comments string. Weed out short comments\n",
        "                    # that are likely just line breaks or something else silly.\n",
        "                    quality = f'{quality}. {details.strip()}'\n",
        "\n",
        "            quality_list.append(quality)\n",
        "\n",
        "            # Convert numbers to \"seconds since\" integers for time indexing\n",
        "            # into quality_flag array.\n",
        "            start_stamp = datetime.strptime(start, format)\n",
        "            end_stamp = datetime.strptime(end, format)\n",
        "\n",
        "            cdm_start, cdm_end = self.get_times([start_stamp, end_stamp])\n",
        "\n",
        "            quality_bins.append({'start': cdm_start, 'end': cdm_end,\n",
        "                                 'low_freq': float(low_freq),\n",
        "                                 'high_freq': float(high_freq),\n",
        "                                 'quality': quality_values[entry[\"quality\"]]})\n",
        "\n",
        "        quality_statement = '; '.join(quality_list)\n",
        "\n",
        "        return quality_statement, quality_bins\n",
        "\n",
        "    @staticmethod\n",
        "    def build_quality(time_length, freq_length):\n",
        "        \"\"\"Build data quality_flag array and attributes\n",
        "\n",
        "        Args:\n",
        "            time_length (int): Length of time axis.\n",
        "            freq_length (int): Length of frequency axis.\n",
        "\n",
        "        Returns:\n",
        "            quality (mupy array): Initialized data quality array with default\n",
        "            values.\n",
        "            quality_attrs (dict)L Dictionary of data quality attributes for\n",
        "            creating xarray variable.\n",
        "        \"\"\"\n",
        "\n",
        "        quality_attrs = {\n",
        "            'long_name': 'Data quality flag',\n",
        "            'standard_name': 'quality_flag',\n",
        "            'comment': '1 = Good, 2 = Not evaluated/Unknown, '\n",
        "                       '3 =  Compromised/Questionable , 4 = Unusable / Bad',\n",
        "            'coverage_content_type': \"qualityInformation\"\n",
        "        }\n",
        "\n",
        "        quality = np.full((time_length, freq_length), fill_value=2,\n",
        "                          dtype=np.int8)\n",
        "\n",
        "        return quality, quality_attrs\n",
        "\n",
        "    def create_acoustic_nc(self, time_stamps, frequencies, efforts, psd,metadata, file_path):\n",
        "\n",
        "        time_stamps= self.time_stamps\n",
        "        frequencies=self.frequencies\n",
        "        efforts=self.efforts\n",
        "        psd=self.psd\n",
        "        metadata=self.metadata\n",
        "        file_path=self.file_path\n",
        "\n",
        "\n",
        "         # Set file path for new netCDF_converters and get Manta export netCDF_\n",
        "        # converters data an xarray.\n",
        "        # Set file path for new netCDF file in the base directory Output folder\n",
        "        # base_dir = \"/content/drive/My Drive/SoundScape Conversion files\"  # Replace with your base directory\n",
        "\n",
        "\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)  # Create the Output folder if it doesn't exist\n",
        "\n",
        "        self.out_file = os.path.join(output_dir, file_path.split(os.sep)[-1].replace('.csv', '.nc'))\n",
        "        self.out_file = self.out_file.replace(os.path.join('csv', ''), os.path.sep)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Try to get metadata from MANTA export file. This file is not always\n",
        "        # included so set manta_ds to None if that is the case.\n",
        "        try:\n",
        "            manta_ds = self.get_manta_netcdf()\n",
        "        except FileNotFoundError:\n",
        "            manta_ds = None\n",
        "            log.warning(f'MANTA metadata export file for {file_path} not '\n",
        "                        f'found')\n",
        "\n",
        "        # Get times from csv_data time_stamp and get string timestamps.\n",
        "        cdm_time = self.get_times(time_stamps)\n",
        "        string_times = [t.isoformat() for t in time_stamps]\n",
        "\n",
        "        # Get time variable attribute dictionaries.\n",
        "        time_stamp_attrs, time_attrs = self.time_variable_attrs(time_stamps,\n",
        "                                                                cdm_time)\n",
        "\n",
        "        # Build frequency attribute dictionary.\n",
        "        freq_attrs = {'actual_range': (frequencies.min(), frequencies.max()),\n",
        "                      'long_name': 'Center frequency of hybrid millidecade '\n",
        "                                   'spectral bands',\n",
        "                      'standard_name': \"sound_frequency\",\n",
        "                      'units': \"Hz\",\n",
        "                      'coverage_content_type': 'coordinate'\n",
        "        }\n",
        "\n",
        "        # Build psd data attribute dictionary.\n",
        "        psd_attrs = {\n",
        "            'long_name': 'Single-sided mean-square sound pressure '\n",
        "                         'spectral density re 1 micropascal^2/Hz',\n",
        "            'standard_name': 'sound_intensity_in_water',\n",
        "            'units': 'dB',\n",
        "            'comment': 'Computation of single-sided mean-square sound '\n",
        "                       'pressure spectral density followed ISO 18405 '\n",
        "                       '3.1.3.13.',\n",
        "            'coverage_content_type': 'physicalMeasurement'\n",
        "        }\n",
        "\n",
        "        effort_attrs = {\n",
        "            'long_name': 'Duration of input data available for each 1-minute '\n",
        "                         'bin',\n",
        "            'units': 'seconds',\n",
        "            'coverage_content_type': \"qualityInformation\"\n",
        "        }\n",
        "\n",
        "        # Create coordinates entry.\n",
        "        coords = {'time': (['time'], cdm_time, time_attrs),\n",
        "                  'frequency': (['frequency'], frequencies, freq_attrs)}\n",
        "\n",
        "        # Get complete metadata entry.\n",
        "        full_metadata, quality_bins = self.get_metadata(metadata)\n",
        "\n",
        "        try:\n",
        "            full_metadata.update({\n",
        "                'PreampFixedGain_dB': manta_ds.PreampFixedGain_dB.item(0),\n",
        "                'SamplingRate': int(manta_ds.SamplingRate.item(0)),\n",
        "                'CalibrationFrequency_Hz': manta_ds.CalibrationFrequency_Hz.\n",
        "                item(0),\n",
        "                'CalibrationSensitivity_dB_re_1VperRefPress': manta_ds.\n",
        "                CalibrationSensitivity_dB_re_1VperRefPress.item(0),\n",
        "                'CalibrationDate': manta_ds.CalibrationDate.item(0).decode()\n",
        "            })\n",
        "        except Exception as e:\n",
        "            log.debug(f'Error \"{e}\" getting calibration data from MANTA '\n",
        "                      f'export netCDF '\n",
        "                      f'{self.out_file.replace(\".nc\", \"_netCDF.nc\")}')\n",
        "\n",
        "        # Build quality_flag data array and attributes\n",
        "        quality, quality_attrs = self.build_quality(cdm_time.shape[0],\n",
        "                                                    frequencies.shape[0])\n",
        "\n",
        "        # Build data variable dictionary for dataset.\n",
        "        data_vars = {'timestamp': (['time'], string_times, time_stamp_attrs),\n",
        "                     'effort': (['time'], efforts, effort_attrs),\n",
        "                     'psd': (['time', 'frequency'], psd, psd_attrs),\n",
        "                     'quality_flag': (['time', 'frequency'], quality,\n",
        "                                      quality_attrs)\n",
        "                     }\n",
        "\n",
        "        new_ds = xr.Dataset(data_vars=data_vars,\n",
        "                            coords=coords,\n",
        "                            attrs=full_metadata)\n",
        "\n",
        "        # Copy calibration data into new_ds.\n",
        "        new_ds['analog_sensitivity'] = self.cal_data['analog_sensitivity']\n",
        "        new_ds['preamp_gain'] = self.cal_data['preamp_gain']\n",
        "        new_ds['recorder_gain'] = self.cal_data['recorder_gain']\n",
        "        new_ds['sensor_sensitivity'] = self.cal_data['sensor_sensitivity']\n",
        "\n",
        "        # Set  custom _FillValue encoding to suppress the\n",
        "        # automatic _FillValue attribute xarray adds when writing to netCDF.\n",
        "        no_fill = {'_FillValue': None}\n",
        "        encoding = {var: no_fill for var in new_ds.data_vars}\n",
        "        encoding.update({var: no_fill for var in new_ds.coords})\n",
        "\n",
        "        print(\"Inital Quality Matrix:\")\n",
        "        print(quality)\n",
        "\n",
        "        # Update quality_flag values.\n",
        "        message = ''\n",
        "        errors = set()\n",
        "        for bin in quality_bins:\n",
        "            # Test time ranges to confirm quality range.\n",
        "            times = [bin['start'], bin['end'], new_ds.time[0],  new_ds.time[-1]]\n",
        "            quality_start, quality_end, time_start, time_end = self.get_dates(\n",
        "                                                                          times)\n",
        "            if time_end < quality_start or time_start > quality_end:\n",
        "                # This file's time range is completely outside the bin\n",
        "                # so skip to next bin.\n",
        "                continue\n",
        "            elif time_start < quality_start:\n",
        "                message = (f'{self.out_file} start {time_start} before quality '\n",
        "                           f'start {quality_start}')\n",
        "            if quality_end < time_end:\n",
        "                message = message + (f'{self.out_file} end {time_end} after '\n",
        "                                     f'quality end {quality_end}')\n",
        "\n",
        "            if message:\n",
        "                log.info(message)\n",
        "\n",
        "            try:\n",
        "                new_ds.quality_flag.loc[\n",
        "                              bin['start']: bin['end'],\n",
        "                              bin['low_freq']:bin['high_freq']] = bin['quality']\n",
        "                print(f\"Updating quality matrix for time: {bin['start']} to {bin['end']} \"\n",
        "                f\"and frequency: {bin['low_freq']} to {bin['high_freq']} \"\n",
        "                f\"with quality value: {bin['quality']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_text = (f'Error \"{e}\" updating quality flag info for '\n",
        "                              f'{self.out_file}')\n",
        "                errors.update([error_text])\n",
        "\n",
        "        if errors:\n",
        "            for error in errors:\n",
        "                log.critical(error)\n",
        "        # If the version number is greater than 1 update the filename with\n",
        "        # the version number. Also check if a previous version is in the\n",
        "        # checksum manifest dictionary and remove it.\n",
        "        version = self.metadata['version']\n",
        "        if version > 1:\n",
        "            if os.path.isfile(self.out_file):\n",
        "                self.old_file = self.out_file\n",
        "\n",
        "            # Get base path by removing .nc extension and csv directory\n",
        "            base_path = os.path.splitext(self.out_file)[0]\n",
        "\n",
        "\n",
        "            # Insert version number before .nc extension\n",
        "            name_parts = base_path.split('_v')\n",
        "            self.out_file = f\"{name_parts[0]}_v{version}.nc\"\n",
        "\n",
        "        print(\" Final Quality Matrix:\")\n",
        "        print(quality)\n",
        "\n",
        "\n",
        "\n",
        "        # Write netCDF_converters file.\n",
        "        log.info(f'Writing {self.out_file}')\n",
        "        try:\n",
        "          new_ds.to_netcdf(self.out_file,  format='NETCDF4',encoding=encoding)\n",
        "        except Exception as e:\n",
        "            log.critical(f'Error \"{e}\" writing {self.out_file}')\n",
        "            print(f'Error \"{e}\" writing {self.out_file}')\n",
        "            raise e\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJtEBsdwTfTp"
      },
      "source": [
        "# `MantaNetCDF` Class\n",
        "\n",
        "The `MantaNetCDF` class extends the `BaseConverter` class to automate the conversion of CSV files into **netCDF files**. It handles metadata, calibration data, and manifest management, ensuring the output files are standardized and version-controlled.\n",
        "\n",
        "### What the Class Does:\n",
        "1. **Automates CSV to netCDF Conversion**:\n",
        "   - Processes CSV files packaged by **PassivePacker** into **netCDF files**.\n",
        "   - Adds the generated netCDF files to a package manifest.\n",
        "   \n",
        "2. **Manages Metadata**:\n",
        "   - Extracts and updates metadata from input files.\n",
        "   - Handles DOIs and integrates citation and acknowledgment data.\n",
        "\n",
        "3. **Handles Calibration Data**:\n",
        "   - Reads calibration metadata from Excel files.\n",
        "   - Updates calibration data for inclusion in the netCDF files.\n",
        "\n",
        "4. **Version Control**:\n",
        "   - Appends version numbers to filenames for datasets with updates.\n",
        "   - Renames and manages older versions to maintain consistency.\n",
        "\n",
        "5. **Error Logging and Manifest Updates**:\n",
        "   - Logs errors and updates manifest files with the latest checksums.\n",
        "\n",
        "\n",
        "\n",
        "### Methods Explanation\n",
        "\n",
        "#### **`__init__()`**\n",
        "- Initializes the class and starts processing CSV files in the base directory.\n",
        "- Iterates through packages, processes metadata, calibration data, and CSV files.\n",
        "- Updates the manifest file and handles errors during processing.\n",
        "\n",
        "#### **`process_package(cal_data)`**\n",
        "- Processes individual packages to create netCDF files.\n",
        "- Integrates calibration data and metadata into the netCDF output.\n",
        "- Updates the manifest file with new netCDF file checksums.\n",
        "\n",
        "#### **`rename_files()`**\n",
        "- Appends version numbers to filenames for updated datasets.\n",
        "- Renames files programmatically and updates the manifest accordingly.\n",
        "\n",
        "#### **`write_manifest(manifest_path)`**\n",
        "- Writes the updated manifest file with new file paths and checksums.\n",
        "\n",
        "#### **`walk_dir(cal_dir)`**\n",
        "- Walks through the specified directory and returns its structure (files and folders).\n",
        "\n",
        "#### **`parse_cal_file()`**\n",
        "- Reads calibration metadata from an Excel file and processes it into structured data for netCDF inclusion.\n",
        "\n",
        "#### **`update_cal_data(raw_cal_data)`**\n",
        "- Updates calibration data with appropriate variable names and attributes for netCDF compatibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1mGi7PPAjgg"
      },
      "outputs": [],
      "source": [
        "# from manta_convert import MantaConvert\n",
        "# from common_tools import utilities as ut\n",
        "# from base_converter import BaseConverter\n",
        "\n",
        "log = logging.getLogger('MANTA')\n",
        "# Configure log to only write once\n",
        "log.handlers = []  # Clear any existing handlers\n",
        "handler = logging.StreamHandler()\n",
        "log.addHandler(handler)\n",
        "\n",
        "\n",
        "class MantaNetCDF(BaseConverter):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Class to automate the conversion product .csv files packaged by\n",
        "        PassivePacker into to netCDF_converters and add file to packages\n",
        "        manifest file.\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Get path to directory containing CSV files to process.\n",
        "        base_path = self.starting_path\n",
        "        directory = os.path.basename(base_path)\n",
        "\n",
        "        start = datetime.now()\n",
        "\n",
        "        # Get a list of potential package directories in the base path.\n",
        "        root, dirs, ignore = next(os.walk(base_dir))\n",
        "\n",
        "        count = 0\n",
        "        errors = 0\n",
        "        for index, package in enumerate(dirs):\n",
        "\n",
        "            self.package_path = os.path.join(root, package)\n",
        "            # Get metadata.\n",
        "            try:\n",
        "                self.get_metadata(metadata_file)\n",
        "            except Exception as e:\n",
        "                if 'No such file or directory' in str(e):\n",
        "                    log.info(f'No metadata file in {package}, '\n",
        "                             f'This is likely not a data package')\n",
        "                else:\n",
        "                    log.critical(f'Error \"{e}\" getting metadata for {package}, '\n",
        "                                 f'Skipping package')\n",
        "                continue\n",
        "\n",
        "            # Look for json files in the data directory\n",
        "            data_dir = os.path.join(self.package_path)\n",
        "\n",
        "            # Update metadata with citation and version info from settings\n",
        "            # file parsed in base converter.\n",
        "            self.metadata['citation'] = self.citation\n",
        "            self.metadata['version'] = self.version\n",
        "            self.metadata['acknowledgement'] = self.acknowledgement\n",
        "\n",
        "            # Handle DOIs. Either use DOI passed in from settings file or\n",
        "            # mint a fresh DOI for this dataset.\n",
        "            if self.doi_minting:\n",
        "                self.mint_doi()\n",
        "            else:\n",
        "                self.metadata['DOI'] = self.doi\n",
        "\n",
        "            # Initialize new manifest dictionary\n",
        "            self.manifest = {}\n",
        "\n",
        "            # Get calibration and other metadata info from calibration file in\n",
        "            # calibration directory.\n",
        "            cal_metadata, raw_cal_data = self.parse_cal_file()\n",
        "\n",
        "            # Update calibration data xarray.\n",
        "            cal_data = self.update_cal_data(raw_cal_data)\n",
        "\n",
        "            # Get a master list of all files in all directories and\n",
        "            # subdirectories under data_files in package.\n",
        "            raw_files = []\n",
        "            for entries in os.walk(data_dir):\n",
        "                files = [os.path.join(entries[0], file) for file in entries[2]\n",
        "                         if '._' not in file]\n",
        "                raw_files.extend(files)\n",
        "\n",
        "            # Process package\n",
        "            try:\n",
        "                self.process_package(cal_data)\n",
        "\n",
        "\n",
        "                # Create new manifest file only if files were successfully processed\n",
        "                if self.manifest:  # Check if manifest dictionary is not empty\n",
        "                    manifest_file = os.path.join(self.package_path, 'manifest-md5.txt')\n",
        "                    self.write_manifest(manifest_file)\n",
        "                    log.info(f'Created manifest file with {len(self.manifest)} entries')\n",
        "                else:\n",
        "                    log.warning('No files were successfully processed, manifest will be empty')\n",
        "\n",
        "                log.info(f'Completed: {package}')\n",
        "                count += 1\n",
        "            except Exception as e:\n",
        "                log.critical(f'Error \"{e}\" processing {package}')\n",
        "                errors += 1\n",
        "\n",
        "        # Processing complete\n",
        "        total_time = datetime.now() - start\n",
        "        log.info(\n",
        "            f'Processing complete. {count} datasets processed in {total_time}')\n",
        "        if errors:\n",
        "            log.info(f'{errors} errors occurred during processing')\n",
        "\n",
        "    def process_package(self, cal_data):\n",
        "        \"\"\"Process the package to create netCDF_converters file and rearrange\n",
        "        files as needed.\n",
        "\n",
        "        Args:\n",
        "            cal_data (DataFrame): Pandas DataFrame containing calibration data.\n",
        "\n",
        "        \"\"\"\n",
        "        dir_path = self.package_path\n",
        "        manifest = self.manifest\n",
        "        metadata = self.metadata\n",
        "\n",
        "        csv_files = csv_file\n",
        "\n",
        "        for file in csv_files:\n",
        "            file_path = file\n",
        "\n",
        "            try:\n",
        "                converter = MantaConvert(file_path, metadata, cal_data,\n",
        "                                         self.version)\n",
        "                # Uncomment these lines to actually process files\n",
        "                # converter.process_csv()\n",
        "                # converter.create_acoustic_nc()\n",
        "\n",
        "                # Add new file to manifest with checksum only if file was created\n",
        "                if hasattr(converter, 'out_file') and os.path.exists(converter.out_file):\n",
        "                    manifest_path = converter.out_file.replace(f'{dir_path}/', '')\n",
        "                    checksum = self.get_checksum(converter.out_file)\n",
        "                    manifest[manifest_path] = checksum.split('  ')[0]\n",
        "                    log.debug(f'Created {manifest_path}')\n",
        "                else:\n",
        "                    log.warning(f'Output file not created for {file_path}')\n",
        "\n",
        "            except IOError as e:\n",
        "                log.critical(e)\n",
        "                continue\n",
        "            except ValueError:\n",
        "                # We've already logged the issue raising this error so just\n",
        "                # skip to next file.\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                log.critical(e)\n",
        "                continue\n",
        "\n",
        "            if not converter:\n",
        "                # We did not want to convert this file so pass.\n",
        "                log.error(f'No converter found for {file_path}')\n",
        "                continue\n",
        "\n",
        "    def write_manifest(self, manifest_path):\n",
        "        \"\"\"Write new manifest file with checksums\n",
        "\n",
        "        Args:\n",
        "            manifest_path (str): Full path for output manifest file\n",
        "\n",
        "        \"\"\"\n",
        "        if not self.manifest:\n",
        "            log.warning('Manifest dictionary is empty, no file will be written')\n",
        "            return\n",
        "\n",
        "        with open(manifest_path, 'w') as m:\n",
        "            for path, checksum in self.manifest.items():\n",
        "                # Make sure path separators are unix style\n",
        "                path = path.replace('\\\\', '/')\n",
        "                m.write(f'{checksum}  {path}\\n')\n",
        "\n",
        "    def walk_dir(self, cal_dir):\n",
        "        \"\"\"Walk through the calibration directory for files.\"\"\"\n",
        "        return next(os.walk(cal_dir), ([], [], []))\n",
        "\n",
        "    def parse_cal_file(self):\n",
        "        \"\"\"Get calibration and other metadata from calibration file.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        cal_metadata = pd.read_excel(cal_file, sheet_name=0)\n",
        "        cal_data = pd.read_excel(cal_file, sheet_name=1)\n",
        "        try:\n",
        "            cal_data.set_index('Frequency_Hz', inplace=True)\n",
        "            cal_data = cal_data.to_xarray()\n",
        "            return cal_metadata, cal_data\n",
        "        except Exception as e:\n",
        "            log.info(f'Calibration file {cal_file} had error \"{e}\". '\n",
        "                        f'Maybe not a proper calibration metadata file? ')\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def update_cal_data(self, raw_cal_data):\n",
        "        \"\"\"Update calibration data xarray Dataset. Changing writable names and\n",
        "        adding attributes\n",
        "\n",
        "        Args:\n",
        "            raw_cal_data (xarray): Raw calibration data xarray pulled from\n",
        "            calibration Excel file.\n",
        "\n",
        "        Returns:\n",
        "            cal_data (xarray): Updated calibration data xarray Dataset\n",
        "\n",
        "        \"\"\"\n",
        "        # Rename variables.\n",
        "        cal_data = raw_cal_data.rename(name_dict={\n",
        "                'Frequency_Hz': 'cal_frequency',\n",
        "                'AnalogSensitivity_dB_re_1VperRefPress': 'analog_sensitivity',\n",
        "                'PreampGain_dB': 'preamp_gain',\n",
        "                'RecorderGain_dB': 'recorder_gain',\n",
        "                'SensorSensitivity_dB_re_1V_perRefPress': 'sensor_sensitivity'})\n",
        "\n",
        "        # Update attributes.\n",
        "        cal_data.cal_frequency.attrs = {\n",
        "                        'long_name': 'Calibration frequency for hybrid '\n",
        "                                     'millidecade spectral bands',\n",
        "                        'units': 'Hz'}\n",
        "        cal_data.analog_sensitivity.attrs = {\n",
        "                            'long_name': 'Analog sensitivity re 1 V per '\n",
        "                                         'reference pressure',\n",
        "                            'units': 'dB',\n",
        "                            'coverage_content_type': 'physicalMeasurement',\n",
        "                            'comment': 'Sensitivity values in dB measured by '\n",
        "                                       'the manufacturer were linearly '\n",
        "                                       'interpolated to the center '\n",
        "                                       'frequencies of hybrid millidecade bands'\n",
        "        }\n",
        "        cal_data.sensor_sensitivity.attrs = {\n",
        "                            'long_name': 'Sensor sensitivity re 1 V per '\n",
        "                                         'reference pressure',\n",
        "                            'units': 'dB',\n",
        "                            'coverage_content_type': 'physicalMeasurement',\n",
        "                            'comment': 'Sensitivity values in dB measured by '\n",
        "                                       'the manufacturer were linearly '\n",
        "                                       'interpolated to the center '\n",
        "                                       'frequencies of HMD bands.'\n",
        "        }\n",
        "        cal_data.preamp_gain.attrs = {'long_name': 'Preamp gain', 'units': 'dB'}\n",
        "        cal_data.recorder_gain.attrs = {'long_name': 'Recorder gain',\n",
        "                                        'units': 'dB'}\n",
        "\n",
        "        return cal_data\n",
        "\n",
        "    def get_checksum(self, file_path):\n",
        "        \"\"\"Calculate MD5 checksum for a file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to file to calculate checksum for\n",
        "\n",
        "        Returns:\n",
        "            str: MD5 checksum string\n",
        "        \"\"\"\n",
        "        import hashlib\n",
        "        md5_hash = hashlib.md5()\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            # Read file in chunks to handle large files efficiently\n",
        "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "                md5_hash.update(chunk)\n",
        "        return md5_hash.hexdigest()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Progress Bar\n",
        "\n",
        "###  Tracking File Processing Progress\n",
        "\n",
        "This code processes a set of folders containing different types of files (like CSV, metadata, calibration, and settings files) one by one. It uses a progress bar to show how many folders have been processed and displays the name of the current folder along with the time it took to process. The `MantaNetCDF()` function handles the actual file processing for each folder."
      ],
      "metadata": {
        "id": "956FQp5cbLIz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPKm6tVb9qTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c8994fa9af4b46dd97f328e523245654",
            "466f45adb4094f02860679b7fc0f04c4",
            "730748a84dc24045a70463882c5f2c33",
            "9b9e9ddbb58a4efcaa3481de6a148782",
            "c1550f1b227e4b1baabd487843a57652",
            "16b9c3009ede4f919da4ead066bcaf58",
            "bb4516aa888545ad90eda78ac4f16369",
            "b9172687416848f3937d1d51760bb8cc",
            "6cf4373ffdac41509cc3c15f4bccaaff",
            "631544ea8b044a6f90396cc157da267e",
            "bad2ec1152ca403082387b41284a4350"
          ]
        },
        "outputId": "545743e1-0b58-49eb-aa99-e66fdc2f4386"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files:   0%|          | 0/1 [00:00<?, ?folder/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8994fa9af4b46dd97f328e523245654"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging is set up. Logs will be written to /content/logBasePath/app.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-2ab97b5e7c41>:70: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
            "  raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
            "<ipython-input-14-2ab97b5e7c41>:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  first_effort = no_dups.iloc[index - 1][1]\n",
            "<ipython-input-14-2ab97b5e7c41>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  second_effort = no_dups.iloc[index][1]\n",
            "Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "2024-12-28 05:35:27,283 - MANTA - INFO - Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "INFO:MANTA:Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n",
            " Final Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-2ab97b5e7c41>:70: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
            "  raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
            "<ipython-input-14-2ab97b5e7c41>:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  first_effort = no_dups.iloc[index - 1][1]\n",
            "<ipython-input-14-2ab97b5e7c41>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  second_effort = no_dups.iloc[index][1]\n",
            "Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "2024-12-28 05:35:30,433 - MANTA - INFO - Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "INFO:MANTA:Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n",
            " Final Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Created manifest file with 2 entries\n",
            "2024-12-28 05:35:30,954 - MANTA - INFO - Created manifest file with 2 entries\n",
            "INFO:MANTA:Created manifest file with 2 entries\n",
            "Completed: settings file\n",
            "2024-12-28 05:35:30,961 - MANTA - INFO - Completed: settings file\n",
            "INFO:MANTA:Completed: settings file\n",
            "<ipython-input-14-2ab97b5e7c41>:70: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
            "  raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
            "<ipython-input-14-2ab97b5e7c41>:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  first_effort = no_dups.iloc[index - 1][1]\n",
            "<ipython-input-14-2ab97b5e7c41>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  second_effort = no_dups.iloc[index][1]\n",
            "Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "2024-12-28 05:35:33,692 - MANTA - INFO - Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "INFO:MANTA:Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200916_DAILY_MILLIDEC_MinRes_v3.nc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n",
            " Final Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-2ab97b5e7c41>:70: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
            "  raw_data = pd.read_csv(self.file_path, parse_dates=parse_dates)\n",
            "<ipython-input-14-2ab97b5e7c41>:89: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  first_effort = no_dups.iloc[index - 1][1]\n",
            "<ipython-input-14-2ab97b5e7c41>:90: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  second_effort = no_dups.iloc[index][1]\n",
            "Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "2024-12-28 05:35:38,082 - MANTA - INFO - Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n",
            "INFO:MANTA:Writing /content/drive/My Drive/MantaConvert/NRS01/NRS01B/Output/NRS01_2022_H5R6.1.5000_20200917_DAILY_MILLIDEC_MinRes_v3.nc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inital Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n",
            " Final Quality Matrix:\n",
            "[[2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " ...\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]\n",
            " [2 2 2 ... 2 2 2]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Created manifest file with 2 entries\n",
            "2024-12-28 05:35:38,336 - MANTA - INFO - Created manifest file with 2 entries\n",
            "INFO:MANTA:Created manifest file with 2 entries\n",
            "Completed: NRS01B\n",
            "2024-12-28 05:35:38,346 - MANTA - INFO - Completed: NRS01B\n",
            "INFO:MANTA:Completed: NRS01B\n",
            "Processing complete. 2 datasets processed in 0:00:15.649598\n",
            "2024-12-28 05:35:38,353 - MANTA - INFO - Processing complete. 2 datasets processed in 0:00:15.649598\n",
            "INFO:MANTA:Processing complete. 2 datasets processed in 0:00:15.649598\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "total_folders = len(folder_files)\n",
        "\n",
        "with tqdm(total=total_folders, desc=\"Processing Files\", unit=\"folder\") as pbar:\n",
        "    for folder, files in folder_files.items():\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Process the current folder\n",
        "        csv_file = files[\"data\"]\n",
        "        metadata_file = files['metadata_file']\n",
        "        cal_file = files['calibration_file']\n",
        "        settings_file = files['settings_file']\n",
        "        output_dir = files['output_path']\n",
        "\n",
        "        MantaNetCDF()\n",
        "\n",
        "        # Update progress bar\n",
        "        elapsed = time.time() - start_time\n",
        "        pbar.set_postfix(current_folder=folder, elapsed=f\"{elapsed:.2f}s\")\n",
        "        pbar.update(1)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8994fa9af4b46dd97f328e523245654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_466f45adb4094f02860679b7fc0f04c4",
              "IPY_MODEL_730748a84dc24045a70463882c5f2c33",
              "IPY_MODEL_9b9e9ddbb58a4efcaa3481de6a148782"
            ],
            "layout": "IPY_MODEL_c1550f1b227e4b1baabd487843a57652"
          }
        },
        "466f45adb4094f02860679b7fc0f04c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b9c3009ede4f919da4ead066bcaf58",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4516aa888545ad90eda78ac4f16369",
            "value": "Processing Files: 100%"
          }
        },
        "730748a84dc24045a70463882c5f2c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9172687416848f3937d1d51760bb8cc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cf4373ffdac41509cc3c15f4bccaaff",
            "value": 1
          }
        },
        "9b9e9ddbb58a4efcaa3481de6a148782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_631544ea8b044a6f90396cc157da267e",
            "placeholder": "​",
            "style": "IPY_MODEL_bad2ec1152ca403082387b41284a4350",
            "value": " 1/1 [00:15&lt;00:00, 15.74s/folder, current_folder=NRS01B, elapsed=15.67s]"
          }
        },
        "c1550f1b227e4b1baabd487843a57652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16b9c3009ede4f919da4ead066bcaf58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4516aa888545ad90eda78ac4f16369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9172687416848f3937d1d51760bb8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf4373ffdac41509cc3c15f4bccaaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "631544ea8b044a6f90396cc157da267e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad2ec1152ca403082387b41284a4350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}